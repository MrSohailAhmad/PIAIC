{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSohailAhmad/PIAIC/blob/piaic-assignments/assignment_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u1rVG0ibIrku",
        "outputId": "1a344b11-9240-401a-bd49-b1cdd00fc057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-genai) (2.27.0)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from google-genai) (11.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-genai) (2.10.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.10/dist-packages (from google-genai) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2024.12.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "# !pip install --upgrade --quiet google-genai\n",
        "!pip install --upgrade google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tMziswnJD5p",
        "outputId": "5b345a8d-ba3e-4e30-96a3-e676ba62facc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key found\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    print(\"Key found\")\n",
        "else:\n",
        "    print(\"Key not found\")\n",
        "\n",
        "# GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9eRlovWJJ5cu"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import Client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "model=\"gemini-2.0-flash-exp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IjSZRYt6K3Si"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import GenerateContentResponse\n",
        "from IPython.display import display, Markdown, Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "QM3fZrgyK9D9",
        "outputId": "2232763c-fd19-43a4-e728-3bf546e504a1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "That's a great question! It's a complex topic, but let's break down how AI works in a way that's easier to understand. Instead of focusing on the technical jargon, we'll talk about the core concepts.\n",
              "\n",
              "Think of AI as trying to get computers to learn and solve problems like humans do (or even better in some cases!). Here's a simplified overview of the main components:\n",
              "\n",
              "**1. Data: The Foundation of AI**\n",
              "\n",
              "* **Data is the \"fuel\" for AI.** Just like humans learn from experiences, AI systems learn from data.\n",
              "* **Different Types of Data:** This can include text, images, videos, audio, numbers, sensor readings, and much more.\n",
              "* **The More Data, the Better (Usually):** Generally, the more relevant and high-quality data an AI system has, the better it can learn and perform.\n",
              "\n",
              "**2. Algorithms: The Learning Rules**\n",
              "\n",
              "* **Algorithms are sets of instructions.** They're like recipes that tell the computer how to process data and learn from it.\n",
              "* **Machine Learning:** A crucial type of algorithm that allows AI systems to learn from data *without explicit programming*. This is often where the \"intelligence\" comes from.\n",
              "* **Different Types of Machine Learning:**\n",
              "    * **Supervised Learning:** The AI is given labeled data (e.g., pictures of cats with the label \"cat\"). It learns to predict the label for new, unseen data.\n",
              "    * **Unsupervised Learning:** The AI is given unlabeled data and tries to find patterns and structures within it (e.g., grouping customers based on their purchasing behavior).\n",
              "    * **Reinforcement Learning:** The AI learns by trial and error, receiving rewards or penalties based on its actions (e.g., a game-playing AI learning to win).\n",
              "* **Deep Learning:** A powerful type of machine learning that uses artificial neural networks (inspired by the structure of the human brain) to learn very complex patterns from massive amounts of data. This is behind many of the recent advancements in AI.\n",
              "\n",
              "**3. Models: The Learned Representations**\n",
              "\n",
              "* **A model is the result of the learning process.** It's like a mathematical representation of the patterns and relationships that the AI has discovered in the data.\n",
              "* **Example:** An image recognition model might store patterns of edges, shapes, and colors that it has learned are associated with a particular object (e.g., a cat).\n",
              "* **Using the Model:** Once a model is trained, it can be used to make predictions or decisions on new, unseen data.\n",
              "\n",
              "**4. Processing Power: The Engine**\n",
              "\n",
              "* **AI requires a lot of computational power.** Processing large amounts of data and training complex models is very resource-intensive.\n",
              "* **GPUs (Graphics Processing Units) are often used** because they can perform many calculations in parallel, speeding up the training process.\n",
              "* **Cloud computing platforms** provide access to the necessary resources for AI development and deployment.\n",
              "\n",
              "**Analogy: Training a Dog**\n",
              "\n",
              "Let's use an analogy of training a dog:\n",
              "\n",
              "* **Data:** You show your dog many pictures of balls and tell them \"ball.\"\n",
              "* **Algorithm:** The training method you use is like the machine learning algorithm.\n",
              "* **Model:** The dog's understanding of what a ball is (based on its training) becomes its internal model.\n",
              "* **Prediction:** The dog sees a new ball and brings it to you.\n",
              "\n",
              "**In Summary:**\n",
              "\n",
              "1. **Gather Data:** Collect relevant data that is used to train the AI model.\n",
              "2. **Choose an Algorithm:** Decide which learning method is best for your needs.\n",
              "3. **Train the Model:** The algorithm learns patterns from the data and creates a model.\n",
              "4. **Test and Refine:** Evaluate the model's performance and make adjustments to improve it.\n",
              "5. **Deploy the Model:** Use the trained model to solve problems and make predictions on new data.\n",
              "\n",
              "**Key Things to Remember:**\n",
              "\n",
              "* **AI is not magic.** It's based on mathematical algorithms and statistical methods.\n",
              "* **AI is constantly evolving.** Research is constantly pushing the boundaries of what's possible.\n",
              "* **AI is not a single thing.** There are many different types of AI, each with its own strengths and weaknesses.\n",
              "* **AI is a tool.** It can be used for good or bad, depending on how it's implemented.\n",
              "\n",
              "**Is it perfect?** No, not yet! AI can be prone to biases present in the data it learns from, and it might make mistakes. It's important to understand both the power and limitations of AI.\n",
              "\n",
              "This is a simplified overview, but hopefully, it gives you a good understanding of the fundamental concepts behind how AI works. Do you have any more specific questions about AI that I can help you with? Perhaps you'd like to learn more about a particular type of AI or its application?\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response:GenerateContentResponse = client.models.generate_content(\n",
        " model=model,\n",
        " contents='How does AI work?'\n",
        "\n",
        ")\n",
        "# response\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZyNTeYo2DJ7s",
        "outputId": "ef8b0345-c541-40f8-c635-c6551272166b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dcfe301f-447a-414e-90c2-207406ee31bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dcfe301f-447a-414e-90c2-207406ee31bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving VID_20241224_010932.mp4 to VID_20241224_010932.mp4\n"
          ]
        }
      ],
      "source": [
        "# !wget my_video.mp4 -O my_video.mp4 -q\n",
        "#  upload video from google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7OoES2-5R-x",
        "outputId": "b8cf0aca-6fdc-4114-ca6c-e05cbc69c389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/xrekv0dlnpo\n"
          ]
        }
      ],
      "source": [
        "# Upload all the videos using the File API.\n",
        "# You can find more details about how to use it in the Get Started notebook.\n",
        "# This can take a couple of minutes as the videos will need to be processed and tokenized.\n",
        "\n",
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(path=video_file_name)\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name or \"\")\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + (video_file.uri or \"\"))\n",
        "\n",
        "  return video_file\n",
        "\n",
        "my_video = upload_video(\"VID_20241224_010932.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "DsH0e54M5m1c",
        "outputId": "83be6738-c68f-49ee-dfbd-5b1be585f111"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```json\n",
              "[\n",
              "  {\n",
              "    \"timecode\": \"0:00\",\n",
              "    \"caption\": \"A person wearing a black hoodie with a red logo and drawstring is centered in the frame. They have dark hair and a beard, and appear to be talking.  The background consists of what looks like a tiled wall. The text spoken is, “Hi dad I am Mohammad Suhail”\"\n",
              "  },\n",
              "  {\n",
              "     \"timecode\": \"0:04\",\n",
              "     \"caption\": \"The same person from the previous scene is still in the frame, looking directly at the camera.  They continue speaking.  The spoken text is, “and I am a full-stack developer and now I'm learning a gentic chain generative AI.”\"\n",
              "  },\n",
              "  {\n",
              "    \"timecode\": \"0:11\",\n",
              "    \"caption\": \"The same individual remains centered and looks at the camera.  They are still speaking. The text spoken is, “Suggest me what content I should learn in agentic AI.”\"\n",
              "   },\n",
              "   {\n",
              "    \"timecode\": \"0:17\",\n",
              "    \"caption\": \"The speaker continues to face the camera. The text spoken is, “or generative AI”\"\n",
              "  },\n",
              "  {\n",
              "    \"timecode\": \"0:20\",\n",
              "    \"caption\": \"The person is still in the frame and speaking. Their hands come up to the red drawstrings of their hoodie. The spoken text is, “Suggest me some”\"\n",
              "    },\n",
              "   {\n",
              "     \"timecode\": \"0:24\",\n",
              "      \"caption\":\"The speaker continues to hold the red drawstrings and still speaks. The spoken text is, “Suggest me some content that I learn agentic AI or”\"\n",
              "    },\n",
              "  {\n",
              "   \"timecode\":\"0:28\",\n",
              "    \"caption\":\"The person is still looking at the camera and still speaking. The text spoken is, “generative AI”\"\n",
              "   }\n",
              "]\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.genai.types import Content, Part\n",
        "prompt = \"\"\"For each scene in this video,\n",
        "            generate captions that describe the scene along with any spoken text placed in quotation marks.\n",
        "            Place each caption into an object with the timecode of the caption in the video.\n",
        "         \"\"\"\n",
        "\n",
        "video = my_video\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=[\n",
        "        Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                Part.from_uri(\n",
        "                    file_uri=video.uri or \"\",\n",
        "                    mime_type=video.mime_type or \"\"),\n",
        "                ]),\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aDj3f_KFPnaV",
        "outputId": "43b5079e-b7db-49b2-af74-149a48942bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis Response:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Okay, here's an analysis of the video you've provided:\n",
              "\n",
              "**1. Summary of Visual Elements:**\n",
              "\n",
              "*   **Scene:** The video is a close-up shot of a man's face and upper torso. He appears to be indoors, against a plain white or off-white tiled wall, with a wooden slat or strip visible at the top of the frame. There might be a blue towel in the background.\n",
              "*   **Action:** The man is looking directly at the camera and speaking. He adjusts the drawstrings on his hoodie at the end of his request.\n",
              "*   **Objects:** The man is wearing a black hoodie with the word \"NIKE\" prominently displayed in red. He also has white wireless earbuds in his ears. The most prominent object is the red drawstring of his hoodie, as he fiddles with it.\n",
              "*   **Camera Angle:** The camera angle is direct, straight at the man, and fairly close, showing his face and upper chest. It is slightly askew, suggesting the video was taken while the camera was tilted. The man is slightly rotated, as well.\n",
              "*   **Lighting:** The lighting is bright, primarily coming from the front, causing the man's face to be well-lit with some natural shadows and also revealing any marks on the man's face. \n",
              "*   **Composition:** The man is the central focus of the frame. The background is simple and unobtrusive. \n",
              "\n",
              "**2. Summary of Audio Content:**\n",
              "\n",
              "*   **Spoken Words:** The man begins by saying, \"Hi Dad, I am Mohammad Suhail.\" He continues:\n",
              "    * \"And I am a full stack developer and now I am learning agentic AI, generative AI.\"\n",
              "    *  \"Suggest me what content I should learn in Agentic AI or Generative AI.\"\n",
              "    * \"Suggest me some content that I learn in Agentic AI or Generative AI.\"\n",
              "*   **Tone:** The man's tone is conversational, casual, and slightly inquisitive. He sounds like he is genuinely seeking advice and is interested in the content suggestions. His speech has a slight accent (likely from South Asia or the Middle East).\n",
              "*   **Background Sounds:** There is no significant background sound. The audio is clear and focused on the man's voice.\n",
              "\n",
              "**In Summary**\n",
              "\n",
              "The video is a short, personal clip of a man named Mohammad Suhail, who identifies as a full-stack developer, seeking suggestions on what to learn in Agentic or Generative AI. The visual focus is directly on him, and the audio captures his request clearly. He has a casual and friendly tone throughout his request. The background is simple, which allows the viewer to focus on him and his words."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Interaction Response:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Okay, here are the answers to your questions based on the analysis:\n",
              "\n",
              "**1. What is the main message or theme of the video?**\n",
              "\n",
              "The main message of the video is a request for guidance and learning resources related to Agentic and Generative AI. Mohammad Suhail, the speaker, is a full-stack developer who is currently exploring these areas and is directly seeking suggestions from his father (implied by his opening line, \"Hi Dad\"). The core theme is one of **learning and seeking expert advice** in a specific technical domain.\n",
              "\n",
              "**2. How do the visual and audio components complement each other in conveying the message?**\n",
              "\n",
              "The visual and audio components work well together to convey the message:\n",
              "\n",
              "*   **Visuals:** The close-up shot of Mohammad Suhail, looking directly at the camera, creates a sense of intimacy and directness. It makes the viewer feel like they are being personally addressed. His casual attire (hoodie, earbuds) signals a relaxed, informal tone. The simple background ensures that there is no distraction from his request. His manipulation of the hoodie strings at the end might imply a slight bit of nervousness, perhaps, when asking for advice.\n",
              "*   **Audio:** The clear audio focus on his voice ensures that the message is easily understandable. His conversational and inquisitive tone emphasizes his earnest desire to learn. The specific mention of \"Agentic AI\" and \"Generative AI\" immediately establish the context of the request. The slightly accented English, while being a characteristic, is completely clear and does not hinder comprehension.\n",
              "\n",
              "In essence, the visuals put a face to the request, while the audio clearly articulates the specific need. The combination makes the request personal, focused, and easily understandable.\n",
              "\n",
              "**3. Are there any notable emotional tones or themes in the spoken content?**\n",
              "\n",
              "Yes, there are some notable emotional tones and themes in his spoken content:\n",
              "\n",
              "*   **Inquisitiveness:** The primary tone is one of inquisitiveness. He is seeking suggestions and clearly interested in learning more about the AI topics.\n",
              "*   **Respect/Affection (implied):** By addressing his father (\"Hi Dad\"), there is an underlying tone of respect, and potentially even affection. There is an assumed relationship where he values his father's opinion and guidance.\n",
              "*   **Humility:** While he is a full-stack developer, he acknowledges that he is still learning in these new areas, suggesting a humility in his approach to knowledge acquisition.\n",
              "*   **Earnestness:** His tone is genuine, showing he really does want to get the resources needed to begin his studies.\n",
              "\n",
              "**4. What he requests and how to respond to it?**\n",
              "\n",
              "*   **Request:** Mohammad Suhail requests suggestions for content to learn in Agentic AI or Generative AI. He specifically asks what content he should learn in these areas. This is essentially a request for recommendations on learning resources, such as courses, books, tutorials, or specific topics to focus on.\n",
              "\n",
              "*   **How to Respond:** Here's how to respond to his request, keeping his perspective and the context in mind:\n",
              "\n",
              "    1.  **Acknowledge his effort:** Start by acknowledging his initiative. For example, \"Hi Mohammad, it's great to see you're exploring Agentic and Generative AI!\"\n",
              "    2.  **Provide structured recommendations:**  Rather than listing everything at once, it is a good idea to categorize recommendations. You could offer:\n",
              "        *   **Foundational Concepts:** Recommend resources on the basic principles of AI, machine learning, and deep learning if he needs a review. This might include courses from platforms like Coursera, edX, or Khan Academy. Specifically, mention some courses on Neural Networks or Deep Learning basics before diving into Agentic AI.\n",
              "        *   **Agentic AI Resources:** Suggest specific materials on Agentic AI (also known as autonomous agents or intelligent agents). Examples could include research papers, blogs, or courses focusing on this. Some key aspects would be to understand the concepts of environments, states, actions, policies, rewards, and RL (Reinforcement Learning).\n",
              "        *   **Generative AI Resources:** Offer recommendations on Generative AI, such as courses or resources covering GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), or diffusion models, as well as practical frameworks such as Transformers.\n",
              "        *   **Specific Tools/Frameworks:** Suggest specific libraries and frameworks that are relevant to his interests, such as TensorFlow, PyTorch, or specialized libraries for RL or Generative AI, and provide links to tutorials.\n",
              "        *   **Project Ideas:** Propose some project ideas. This can be crucial for him to get hands-on experience.\n",
              "        *   **Staying Up-To-Date:** Mention that the AI landscape is dynamic and it is essential to follow the research and updates in the field, providing suggestions on blogs or journals to follow.\n",
              "    3.  **Offer further assistance:** Encourage him to ask further questions and offer to help as he progresses. For instance, \"Let me know if you have more questions along the way.\" or \"If you get stuck on something feel free to reach out\".\n",
              "    4.  **Personalized touch:** You can tailor the response further if you have an understanding of his current skill levels and preferences.\n",
              "\n",
              "By using this approach, the response will be helpful, well-structured, and encouraging. It directly addresses the user's request and fosters further engagement.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Analyze the video (visual and audio components)\n",
        "def analyze_video(video_file):\n",
        "    \"\"\"\n",
        "    Analyzes both the visual and audio components of the uploaded video.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    Analyze the uploaded video and provide:\n",
        "    1. A summary of the visual elements (e.g., scenes, actions, objects).\n",
        "    2. A summary of the audio content, including spoken words, tone, and any significant background sounds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=[\n",
        "                Content(\n",
        "                    role=\"user\",\n",
        "                    parts=[\n",
        "                        Part.from_uri(\n",
        "                            file_uri=video_file.uri or \"\",\n",
        "                            mime_type=video_file.mime_type or \"\"\n",
        "                        )\n",
        "                    ]\n",
        "                ),\n",
        "                prompt\n",
        "            ]\n",
        "        )\n",
        "        print(\"Analysis Response:\")\n",
        "        display(Markdown(response.text))\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing video: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Call the analysis function\n",
        "analysis_result = analyze_video(my_video)\n",
        "\n",
        "# Interact with the LLM based on the analysis\n",
        "def interact_with_llm(analysis_text):\n",
        "    \"\"\"\n",
        "    Interacts with the LLM by asking questions based on the video analysis.\n",
        "    \"\"\"\n",
        "    question_prompt = \"\"\"\n",
        "    Based on the provided analysis, answer the following:\n",
        "    1. What is the main message or theme of the video?\n",
        "    2. How do the visual and audio components complement each other in conveying the message?\n",
        "    3. Are there any notable emotional tones or themes in the spoken content?\n",
        "    5. what he request and how to respond to it?\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=[\n",
        "                analysis_text,\n",
        "                question_prompt\n",
        "            ]\n",
        "        )\n",
        "        print(\"LLM Interaction Response:\")\n",
        "        display(Markdown(response.text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM interaction: {e}\")\n",
        "\n",
        "\n",
        "# Proceed if analysis was successful\n",
        "if analysis_result:\n",
        "    interact_with_llm(analysis_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMdMpWImP2Mm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMO6Ya7fA/q5kHhVluJA5dY",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
